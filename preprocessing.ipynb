{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24\n",
      "  Downloading numpy-1.24.0-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting scipy==1.13\n",
      "  Downloading scipy-1.13.0-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Collecting smart_open\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Installing collected packages: numpy, scipy, wrapt, smart-open\n",
      "Successfully installed numpy-1.24.0 scipy-1.13.0 smart-open-7.1.0 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.24 scipy==1.13 smart_open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==4.3.3\n",
      "  Downloading gensim-4.3.3-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim==4.3.3) (1.24.0)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim==4.3.3) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim==4.3.3) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.17.2)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim==4.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4; python_version < \"3.11\" in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\william\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\william\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\william\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Installing collected packages: joblib, tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.24.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\william\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\william\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\william\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Installing collected packages: fsspec, pyyaml, certifi, charset-normalizer, urllib3, idna, requests, filelock, huggingface-hub, tokenizers, safetensors, transformers\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.0 huggingface-hub-0.29.3 idna-3.10 pyyaml-6.0.2 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.3 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\william\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.13.0)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.18.0)\n",
      "Collecting sympy==1.13.1; python_version >= \"3.9\"\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Installing collected packages: networkx, mpmath, sympy, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\william\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.24.0)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import datasets\n",
    "twitter_train = pd.read_csv('raw data\\\\twitter_training.csv', header=None, names=['Tweet id','topic', 'sentiment','Tweet content'])\n",
    "twitter_val = pd.read_csv('raw data\\\\twitter_validation.csv', header=None, names=['Tweet id','topic', 'sentiment','Tweet content'])\n",
    "twitter = pd.concat([twitter_train, twitter_val], ignore_index=True)\n",
    "emotion_text = pd.read_csv('raw data\\\\tweet_emotions.csv')\n",
    "youtube_comments = pd.read_csv('raw data\\\\YoutubeCommentsDataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data (duplicates, missing values, uniform text, etc.)\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        # Remove user mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        # Remove punctuation and special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop irrelevant rows\n",
    "twitter = twitter[twitter['sentiment'] != 'Irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map youtube labels\n",
    "capital = {\n",
    "    'positive': 'Positive',\n",
    "    'negative': 'Negative',\n",
    "    'neutral': 'Neutral'\n",
    "}\n",
    "youtube_comments['Sentiment'] = youtube_comments['Sentiment'].map(capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map emotions to uniform data\n",
    "emotion_to_sentiment = {\n",
    "    'enthusiasm': 'Positive',\n",
    "    'surprise': 'Positive',\n",
    "    'love': 'Positive',\n",
    "    'fun': 'Positive',\n",
    "    'happiness': 'Positive',\n",
    "    'neutral': 'Neutral',\n",
    "    'relief': 'Positive',\n",
    "    'anger': 'Negative',\n",
    "    'boredom': 'Negative',\n",
    "    'hate': 'Negative',\n",
    "    'worry': 'Negative',\n",
    "    'sadness': 'Negative',\n",
    "    'empty': 'Negative'\n",
    "}\n",
    "\n",
    "# Apply mapping to emotion dataset\n",
    "emotion_text['sentiment'] = emotion_text['sentiment'].map(emotion_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "# Twitter dataset\n",
    "twitter['cleaned_text'] = twitter['Tweet content'].apply(clean_text)\n",
    "twitter_subset = twitter[['cleaned_text', 'sentiment']]\n",
    "\n",
    "# YouTube dataset\n",
    "youtube_comments['cleaned_text'] = youtube_comments['Comment'].apply(clean_text)\n",
    "youtube_subset = youtube_comments[['cleaned_text', 'Sentiment']]\n",
    "youtube_subset = youtube_subset.rename(columns={'Sentiment': 'sentiment'})\n",
    "\n",
    "# Emotion dataset\n",
    "emotion_text['cleaned_text'] = emotion_text['content'].apply(clean_text)\n",
    "emotion_subset = emotion_text[['cleaned_text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat data into one dataset\n",
    "combined_data = pd.concat([twitter_subset, youtube_subset, emotion_subset], ignore_index=True)\n",
    "combined_data = combined_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "combined_data = combined_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode data\n",
    "label_encoder = LabelEncoder()\n",
    "combined_data['sentiment_encoded'] = label_encoder.fit_transform(combined_data['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment mapping: {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n"
     ]
    }
   ],
   "source": [
    "# Print the mapping for reference\n",
    "sentiment_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Sentiment mapping:\", sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hybrid Tokenizer class for ECO-SAM, UFEN, and Neuro-Symbolic Transformer with LDA\n",
    "class HybridTokenizer:\n",
    "    def __init__(self, max_length=128, lda_num_topics=10):\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "        self.lda_num_topics = lda_num_topics\n",
    "        self.lda_dictionary = None\n",
    "        self.lda_model = None\n",
    "\n",
    "    def fit_lda(self, texts):\n",
    "        # Tokenize texts for LDA\n",
    "        tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "        self.lda_dictionary = Dictionary(tokenized_texts)\n",
    "        corpus = [self.lda_dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "        self.lda_model = LdaModel(corpus=corpus, id2word=self.lda_dictionary, num_topics=self.lda_num_topics)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # BERT tokenization for ECO-SAM\n",
    "        bert_encoding = self.bert_tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Word tokenization for UFEN\n",
    "        words = word_tokenize(text.lower())\n",
    "\n",
    "        # LDA topic distribution for Neuro-Symbolic Transformer\n",
    "        if self.lda_model is not None:\n",
    "            bow = self.lda_dictionary.doc2bow(words)\n",
    "            topic_dist = self.lda_model.get_document_topics(bow)\n",
    "            # Convert sparse topic distribution to dense\n",
    "            dense_topics = [0] * self.lda_num_topics\n",
    "            for topic_id, prob in topic_dist:\n",
    "                dense_topics[topic_id] = prob\n",
    "            topic_dist = dense_topics\n",
    "        else:\n",
    "            topic_dist = [0] * self.lda_num_topics\n",
    "\n",
    "        return {\n",
    "            'input_ids': bert_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': bert_encoding['attention_mask'].squeeze(),\n",
    "            'words': words,\n",
    "            'topic_dist': torch.tensor(topic_dist, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting borderland murder</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coming border kill</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting borderland kill</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming borderland murder</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spent hour making something fun dont know huge...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text sentiment  \\\n",
       "0                       im getting borderland murder  Positive   \n",
       "1                                 coming border kill  Positive   \n",
       "2                         im getting borderland kill  Positive   \n",
       "3                        im coming borderland murder  Positive   \n",
       "6  spent hour making something fun dont know huge...  Positive   \n",
       "\n",
       "   sentiment_encoded  \n",
       "0                  2  \n",
       "1                  2  \n",
       "2                  2  \n",
       "3                  2  \n",
       "6                  2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA model...\n",
      "Train set: 64563 samples\n",
      "Validation set: 21522 samples\n",
      "Test set: 21522 samples\n",
      "Tokenizing datasets...\n",
      "Tokenization and data splitting completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = HybridTokenizer(max_length=128, lda_num_topics=10)\n",
    "\n",
    "# Fit LDA model on the cleaned text data\n",
    "print(\"Fitting LDA model...\")\n",
    "tokenizer.fit_lda(combined_data['cleaned_text'])\n",
    "\n",
    "# Create train, validation, and test splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create stratified splits to maintain class distribution\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42, stratify=combined_data['sentiment_encoded'])\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=train_data['sentiment_encoded'])\n",
    "\n",
    "print(f\"Train set: {len(train_data)} samples\")\n",
    "print(f\"Validation set: {len(val_data)} samples\")\n",
    "print(f\"Test set: {len(test_data)} samples\")\n",
    "\n",
    "# Function to tokenize and store results\n",
    "def tokenize_and_store(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return {\n",
    "        'input_ids': tokens['input_ids'].tolist(),\n",
    "        'attention_mask': tokens['attention_mask'].tolist(),\n",
    "        'topic_dist': tokens['topic_dist'].tolist()\n",
    "    }\n",
    "\n",
    "# Apply tokenization to each split\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_tokenized = train_data['cleaned_text'].apply(tokenize_and_store)\n",
    "val_tokenized = val_data['cleaned_text'].apply(tokenize_and_store)\n",
    "test_tokenized = test_data['cleaned_text'].apply(tokenize_and_store)\n",
    "\n",
    "# Add tokenized data to each dataframe\n",
    "train_data['input_ids'] = train_tokenized.apply(lambda x: x['input_ids'])\n",
    "train_data['attention_mask'] = train_tokenized.apply(lambda x: x['attention_mask'])\n",
    "train_data['topic_dist'] = train_tokenized.apply(lambda x: x['topic_dist'])\n",
    "\n",
    "val_data['input_ids'] = val_tokenized.apply(lambda x: x['input_ids'])\n",
    "val_data['attention_mask'] = val_tokenized.apply(lambda x: x['attention_mask'])\n",
    "val_data['topic_dist'] = val_tokenized.apply(lambda x: x['topic_dist'])\n",
    "\n",
    "test_data['input_ids'] = test_tokenized.apply(lambda x: x['input_ids'])\n",
    "test_data['attention_mask'] = test_tokenized.apply(lambda x: x['attention_mask'])\n",
    "test_data['topic_dist'] = test_tokenized.apply(lambda x: x['topic_dist'])\n",
    "\n",
    "# Save the dataframes\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "val_data.to_csv('val_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n",
    "# Save tokenized arrays for efficient model training\n",
    "np.save('train_input_ids.npy', np.array(train_data['input_ids'].tolist()))\n",
    "np.save('train_attention_mask.npy', np.array(train_data['attention_mask'].tolist()))\n",
    "np.save('train_topic_dist.npy', np.array(train_data['topic_dist'].tolist()))\n",
    "np.save('train_labels.npy', np.array(train_data['sentiment_encoded']))\n",
    "\n",
    "np.save('val_input_ids.npy', np.array(val_data['input_ids'].tolist()))\n",
    "np.save('val_attention_mask.npy', np.array(val_data['attention_mask'].tolist()))\n",
    "np.save('val_topic_dist.npy', np.array(val_data['topic_dist'].tolist()))\n",
    "np.save('val_labels.npy', np.array(val_data['sentiment_encoded']))\n",
    "\n",
    "np.save('test_input_ids.npy', np.array(test_data['input_ids'].tolist()))\n",
    "np.save('test_attention_mask.npy', np.array(test_data['attention_mask'].tolist()))\n",
    "np.save('test_topic_dist.npy', np.array(test_data['topic_dist'].tolist()))\n",
    "np.save('test_labels.npy', np.array(test_data['sentiment_encoded']))\n",
    "\n",
    "# Save the tokenizer for future use\n",
    "with open('hybrid_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Tokenization and data splitting completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
